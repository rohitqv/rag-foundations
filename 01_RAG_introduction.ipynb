{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# RAG Introduction — Notes\n",
        "\n",
        "This notebook contains foundational notes on **Retrieval-Augmented Generation (RAG)**, classical search, LLMs, and design choices for building high-performing RAG systems."
      ],
      "id": "f974c246"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Classical Search Systems\n",
        "\n",
        "- **Classical search** refers to traditional information retrieval (IR) systems that existed before LLMs.\n",
        "- The broader field of **information retrieval** was already mature when large language models were first developed.\n",
        "- Ideas from this field underlie how **retrievers** and **RAG systems** are designed.\n",
        "- At scale, most retrievers are based on **vector databases**.\n",
        "- There is a **variety of approaches** for computing similarity scores between queries and documents."
      ],
      "id": "9ed0ff01"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Foundational Concepts: Search and LLM\n",
        "\n",
        "- Understanding both **search (retrieval)** and **LLM (generation)** is essential for RAG.\n",
        "- **Retrieval** manages a knowledge base of trusted, relevant, and possibly private information.\n",
        "- **Retrievers** search the knowledge base to find content relevant to a user query.\n",
        "- **LLMs** generate text (completions) given a prompt; they benefit from having relevant information added to that prompt."
      ],
      "id": "f28d4f8f"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. What is RAG and Why It Helps\n",
        "\n",
        "- **RAG improves the performance of LLMs** by giving them information they did not see (or see enough of) in training.\n",
        "- When we send a prompt to an LLM, we hope the information relevant to our question was in the training data—ideally many times.\n",
        "- There will always be **information the model was not trained on**. In such cases, we **add that information to the prompt**.\n",
        "- The **retriever** manages a knowledge base of trusted, relevant, and possibly private information and **searches** it to supply that information."
      ],
      "id": "8dd081ef"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Core RAG Building Blocks\n",
        "\n",
        "| Component | Role |\n",
        "|-----------|------|\n",
        "| **Retrieve function** | Wrapper around the retriever. Accepts a text query and returns relevant documents (or chunks) from the knowledge base. |\n",
        "| **Generate function** | Wrapper around the LLM. Accepts a text prompt and returns the LLM response. |\n",
        "\n",
        "RAG pipelines typically: **query → retrieve → build prompt (query + docs) → generate**."
      ],
      "id": "894871ac"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Introduction to LLMs\n",
        "\n",
        "### 5.1 LLMs as “fancy autocomplete”\n",
        "- Under the hood, LLMs use a **neural network**—a large mathematical model of language.\n",
        "- The model encodes which words are commonly used together, in what order, and what they mean in context.\n",
        "- When an LLM generates a completion, it is **adding new tokens to the end of the prompt, one token at a time**.\n",
        "\n",
        "### 5.2 Tokens (not words)\n",
        "- LLMs generate **tokens**, not raw words. A token is a piece of text (subword, word, or punctuation).\n",
        "- Examples: “London” and “door” might be single tokens; “programmatically” and “unhappy” are often split into multiple tokens.\n",
        "- Punctuation (e.g. `.`, `,`, `?`) can be separate tokens.\n",
        "- Typical vocabulary size: **tens of thousands to 100,000+ tokens**. Using subword tokens lets the model represent any word without a token for every possible word.\n",
        "\n",
        "### 5.3 How the next token is chosen\n",
        "1. Process the **current completion** (prompt + already generated tokens) to capture relationships and meaning.\n",
        "2. For **each token in the vocabulary**, compute the **probability** that it should appear next.\n",
        "3. **Sample** the next token from this probability distribution (e.g. “shining” might have high probability, “rising” lower, “exploding” very low but non-zero).\n",
        "4. **Repeat** for the next token. Earlier token choices affect later ones—this is **auto-regressive** behavior and keeps the completion coherent.\n",
        "\n",
        "### 5.4 Context and cost\n",
        "- **Longer prompts** require more computation.\n",
        "- There is a **context window limit**—the maximum sequence length the model can process at once."
      ],
      "id": "e9692a48"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Purpose of the Retriever — The Librarian Analogy\n",
        "\n",
        "- **Purpose:** Supply the LLM with useful information that was **not** available (or not emphasized) in its training data.\n",
        "- **Analogy:** Think of the retriever as a **librarian**:\n",
        "  - The librarian has **organized** books (documents) in sections.\n",
        "  - You ask a **question**; the librarian interprets it and **finds relevant documents**.\n",
        "  - Documents are assigned **similarity scores** based on how related they are to the question.\n",
        "- **Ideal case:** The retriever ranks documents perfectly and returns exactly the right number.\n",
        "- **In practice:** The retriever may rank some relevant documents too low and some irrelevant ones too high, making it hard to know how many to return. **Monitoring and experimentation** (e.g. with similarity thresholds and top-k) are needed to optimize performance."
      ],
      "id": "5155fcc4"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. RAG as a Design Space (Not a Single “Concept”)\n",
        "\n",
        "- **RAG is not one fixed concept**—there are **many ways to implement** it.\n",
        "- **Design choices** directly affect the **accuracy and performance** of a RAG system.\n",
        "- To build a good system, you need to make deliberate choices about retrieval, chunking, prompting, and generation."
      ],
      "id": "cd57d34c"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Attributes of a High-Performing RAG System\n",
        "\n",
        "- **Relevance:** Retrieved documents are actually useful for answering the query.\n",
        "- **Precision vs. recall:** Right balance of how many documents to retrieve and how strict similarity thresholds are.\n",
        "- **Robustness:** Handles ambiguous or noisy queries and documents.\n",
        "- **Efficiency:** Latency and cost (retrieval + LLM) are acceptable for the use case.\n",
        "- **Maintainability:** Easy to update the knowledge base, change models, or tune parameters.\n",
        "- **Observability:** You can monitor retrieval quality and generation quality over time."
      ],
      "id": "466c1573"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Design Choices for RAG\n",
        "\n",
        "- **Chunking strategy:** How to split documents (by paragraph, sentence, fixed size, semantic, etc.).\n",
        "- **Embedding model:** Which model to use for turning text into vectors (affects similarity quality).\n",
        "- **Retrieval method:** Dense (vector) vs. sparse (e.g. BM25) vs. hybrid; filters and metadata.\n",
        "- **Number of documents / top-k:** How many chunks to pass to the LLM.\n",
        "- **Prompt template:** How to combine the query and retrieved documents (order, instructions, format).\n",
        "- **LLM choice:** Model size, instruction-tuning, and cost/latency tradeoffs.\n",
        "- **Re-ranking (optional):** A second-stage model to re-rank retrieved chunks before generation."
      ],
      "id": "62568122"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Tuning a RAG System\n",
        "\n",
        "- **Hyperparameter tuning** is important: e.g. chunk size, overlap, top-k, similarity threshold, temperature for the LLM.\n",
        "- Monitor retrieval metrics (e.g. relevance, recall@k) and generation quality (e.g. faithfulness, answer relevance).\n",
        "- Iterate on **chunking**, **embedding model**, and **prompt design** based on evaluation and user feedback."
      ],
      "id": "1439790f"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Advanced and Extended RAG Topics\n",
        "\n",
        "- **Multi-step, agentic workflows:** RAG is often **one component** in a larger agentic pipeline (e.g. plan → retrieve → reason → act). The agent may call tools, use multiple retrievers, or chain several steps.\n",
        "- **Agentic document extraction:** Using LLM/agent logic to extract, normalize, or structure information from raw documents before or alongside retrieval.\n",
        "- **Multi-model / reasoning models:** Combining different models (e.g. one for retrieval/reranking, one for reasoning, one for generation) or using specialized “reasoning” models in the loop.\n",
        "- **RAG fine-tuning:** Fine-tuning the LLM (or the retriever) on your domain or on “retrieve-then-answer” examples to improve end-to-end RAG performance.\n",
        "- **Long-context models:** Models with very large context windows can reduce the need for perfect retrieval by including more (or all) documents in the prompt; tradeoffs with cost and latency.\n",
        "- **Agentic RAG:** Systems where **multiple LLMs** (or agents) each handle a **single part** of a large workflow. The system has **agency** to decide **what data to retrieve**, when to re-query, and how to combine retrieval with reasoning and generation."
      ],
      "id": "d2dbd98b"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "| Topic | Takeaway |\n",
        "|-------|----------|\n",
        "| **Classical search** | IR is mature; vector DBs and similarity scoring underpin modern retrievers. |\n",
        "| **RAG** | Augments LLMs by adding retrieved, trusted information to the prompt. |\n",
        "| **Retrieve / Generate** | Retrieve = query → relevant docs; Generate = prompt → LLM response. |\n",
        "| **LLMs** | Token-by-token, auto-regressive generation; limited by context window and compute. |\n",
        "| **Retriever** | Like a librarian: organize, score, and return relevant documents; needs tuning. |\n",
        "| **RAG design** | Many design choices; tuning and monitoring are key to high performance. |\n",
        "| **Advanced** | Agentic workflows, multi-model setups, fine-tuning, and long-context models extend basic RAG. |"
      ],
      "id": "ac608e68"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}